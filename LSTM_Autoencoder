# load dataset function
def load_data(path):
    dataset = pd.read_excel(path, header=0, na_filter=True, na_values='Bad Input')
    dataset = dataset.dropna()
    print(dataset.head())
    print('Range', dataset.index.min(), dataset.index.max())
    return dataset


# filter dataset by the time and normalization
def filter_data(dataset, train_start_time, train_end_time, test_start_time, test_end_time):
    # 1) ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Time Ð² datetime Ð¸ Ð´ÐµÐ»Ð°ÐµÐ¼ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð¼ (Ð² Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð½Ðµ Ð¿Ð¾Ð¿Ð°Ð´Ñ‘Ñ‚)
    dataset = dataset.set_index('Time').sort_index()
    print(dataset.head())

    # 2) Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» ÑÑ‚Ñ€Ð¾Ðº (Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ DataFrame, Ð½Ðµ Series)
    X_train_raw = dataset.loc[train_start_time:train_end_time]
    print(X_train_raw.shape)
    X_test_raw = dataset.loc[test_start_time:test_end_time,:]
    print(X_test_raw.shape)

    # 3) ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    scaler = StandardScaler().fit(X_train_raw)
    X_train_scaled = scaler.transform(X_train_raw)
    X_test_scaled = scaler.transform(X_test_raw)

    print(X_test_scaled[0:3])
    return X_train_scaled, X_test_scaled, X_train_raw, X_test_raw


# make sliding windows
def make_seq(X, SEQ):
    return np.array([X[i:i+SEQ] for i in range(len(X)-SEQ+1)])


# LSTM Autoencoder
def train_model(X_train):
    inp = keras.Input(shape=(SEQ, n_feats))
    x = keras.layers.LSTM(64, return_sequences=True)(inp)
    x = keras.layers.LSTM(32, return_sequences=False)(x)
    x = keras.layers.RepeatVector(SEQ)(x)
    x = keras.layers.LSTM(32, return_sequences=True)(x)
    x = keras.layers.LSTM(64, return_sequences=True)(x)
    out = keras.layers.TimeDistributed(keras.layers.Dense(n_feats))(x)
    model = keras.Model(inp, out)
    model.compile(optimizer='adam', loss='mse')
    callbacks = [keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)]
    model.fit(X_train, X_train, epochs=5, batch_size=128, callbacks=callbacks, validation_split=0.1, verbose=1)
    return model


# --- ðŸ“Œ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð´ ---
# ðŸ“ 1. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸
path='//'
dataset = load_data(path)
print(" âœ… Data Loaded Successfully\n")


# ðŸ“ 2. Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
train_start_time = '2023-05-07 16:00:00'
train_end_time = '2023-05-17 05:00:00'
test_start_time = '2023-05-22 01:00:00'
test_end_time =  '2023-06-05 11:00:00'
X_train_scaled, X_test_scaled, X_train_raw, X_test_raw = filter_data(dataset, train_start_time, train_end_time, test_start_time, test_end_time)
test_idx = X_test_raw.index
test_idx = test_idx.floor('s')


# ðŸ“ 3. Sliding window
SEQ=120 # 120x30 sec = 60 minutes
X_train = make_seq(X_train_scaled, SEQ)
X_test = make_seq(X_test_scaled, SEQ)
n_feats = X_train.shape[2]


# ðŸ“ 4. Instatiate LSTM Autoencoder and Train
model = train_model(X_train)


# ðŸ“ 5. ÐžÑˆÐ¸Ð±ÐºÐ¸ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÑƒ
#prediction
recon_train = model.predict(X_train, verbose=0)
recon_test = model.predict(X_test, verbose=0)

#Ð¿Ð¾ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð½Ñ‹Ðµ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (N, Q, F). ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ð» Ð¿Ð¾ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð½ÑƒÑŽ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð½ÑƒÑŽ Ð¾ÑˆÐ¸Ð±ÐºÑƒ (Ð¾ÑˆÐ¸Ð±ÐºÑƒ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸) Ð¼ÐµÐ¶Ð´Ñƒ Ð²Ñ…Ð¾Ð´Ð¾Ð¼ Ð¸ Ð²Ñ‹Ñ…Ð¾Ð´Ð¾Ð¼ Ð°Ð²Ñ‚Ð¾ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ð°.
err_train = (X_train - recon_train)**2
err_test = (X_test - recon_test)**2

#ÑƒÑÑ€ÐµÐ´Ð½ÑÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¾ÑÐ¸ Ð¾ÐºÐ½Ð°, Ð½Ð¾ Ð½Ðµ Ð¿Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼ â†’ (N, F)
mse_train_per_feat = err_train.mean(axis=1)
mse_test_per_feat = err_test.mean(axis=1)

df = pd.DataFrame(mse_test_per_feat, columns=dataset.columns[1:])
df.describe().transpose()


# ðŸ“ 6. ÐŸÐ¾Ñ€Ð¾Ð³Ð¸ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°
# Ð¿Ð¾Ñ€Ð¾Ð³ ÐºÐ°Ðº ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð»ÑŒ Ð¿Ð¾ train (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 99.5%)
q = 0.995
thr_per_feat = np.quantile(mse_train_per_feat, q, axis=0)  # shape: (F,)
# Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð°:
# thr_per_feat = mse_train_per_feat.mean(axis=0) + 3*mse_train_per_feat.std(axis=0)
df_thr=pd.Series(thr_per_feat, index=dataset.columns[1:])


# ðŸ“ 7. ÐŸÑ€Ð¸Ð²ÑÐ·ÐºÐ° Ðº Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸ ÑÐ±Ð¾Ñ€ Series Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ ÑÐµÐ½ÑÐ¾Ñ€Ñƒ. Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ ÑˆÐºÐ°Ð»Ð¾Ð¹
feat_names = dataset.columns[1:].tolist()
Q = X_test.shape[1]
idx = test_idx[Q-1 : Q-1 + len(mse_test_per_feat)]  # Ð´Ð»Ð¸Ð½Ð° = Nte - ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐºÐ¾Ð»ÑŒÐ·ÑÑ‰Ð¸Ñ… Ð¾ÐºÐ¾Ð½

# Ð¡Ð»Ð¾Ð²Ð°Ñ€Ð¸: Ð¸Ð¼Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ° -> Series (Ð¸Ð½Ð´ÐµÐºÑ Ð²Ñ€ÐµÐ¼Ñ, Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ 0/1 Ð¸Ð»Ð¸ score)
anomaly_series_by_feat = {}
mse_series_by_feat   = {}

for j, name in enumerate(feat_names):
    anomaly_series_by_feat[name] = pd.Series(mse_test_per_feat[:, j].astype(int), index=idx)
    mse_series_by_feat[name]   = pd.Series(mse_test_per_feat[:, j], index=idx)


# ðŸ“ 8.  Health index function for parameteres. Normalization.
def health_index(parameter):
  mse_series_health = mse_series_by_feat[parameter].copy()
  HI = mse_series_health.ewm(span=96, adjust=False).mean() # smooths your anomaly scores with an exponentially weighted moving average

  # normalize relative to healthy baseline (before onset)
  baseline_window = HI.index.min(), HI.index.min() + pd.Timedelta(days=2) # baseline_window defines a 2-day interval at the very beginning of your data.
  baseline = HI.loc[slice(*baseline_window)].quantile(0.95) #Ð’ Ð¸Ñ‚Ð¾Ð³Ðµ baseline = 95-Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð¸Ð»ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ HI Ð·Ð° Ð¿ÐµÑ€Ð²Ñ‹Ðµ Ð´Ð²Ð° Ð´Ð½Ñ,
  #ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸Ð»Ð¸ ÐºÐ°Ðº Ð¿Ð¾Ñ€Ð¾Ð³ "Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ".
  HI_norm = ((HI - baseline).clip(lower=0))/(HI.quantile(0.999) - baseline)
  HI_norm = HI_norm.clip(0, 1) # normalized from 0 to 1 # turning it into a normalized score in the range [0, 1]
  return HI_norm


# ðŸ“ 9. health index function for parameters
def health_index_multi(parameters):
    HI_dict = {}
    for param in parameters:
        HI_dict[param] = health_index(param)
    return HI_dict


# ðŸ“ 10. compute health index
Vibr =r'tag'
P2 =  r'tag'
Curr = r'tag'
Flow = r'tag'
params = [Vibr, P2, Curr, Flow]
HI_dict = health_index_multi(params)

# ðŸ“ 11. compute persist of the health index
HI_persist = {}
for param in params:
  HI_persist[param] = (HI_dict[param].rolling(60, min_periods=60).mean() >= 0.8).astype(int)


# ðŸ“ 12. compute presence of parameters anomaly detection
anomaly_detection = {}

if (HI_persist[Vibr] >= 1.0).any():
    anomaly_detection['Vibration'] = (True, HI_persist[Vibr][HI_persist[Vibr] == 1].index[0])
else:
    anomaly_detection['Vibration'] = (False, None)

if (HI_persist[P2] >= 1.0).any():
    anomaly_detection['P2'] = (True, HI_persist[P2][HI_persist[P2] == 1].index[0])
else:
    anomaly_detection['P2'] = (False, None)

if (HI_persist[Curr] >= 1.0).any():
    anomaly_detection['Motor_Current'] = (True, HI_persist[Curr][HI_persist[Curr] == 1].index[0])
else:
    anomaly_detection['Motor_Current'] = (False, None)

if (HI_persist[Flow] >= 1.0).any():
    anomaly_detection['Flow'] = (True, HI_persist[Flow][HI_persist[Flow] == 1].index[0])
else:
    anomaly_detection['Flow'] = (False, None)


print (anomaly_detection)
{'Vibration': (True, Timestamp('2023-06-05 06:54:31')),
 'P2': (True, Timestamp('2023-06-04 19:16:54')),
 'Motor_Current': (True, Timestamp('2023-06-04 19:14:54')),
 'Flow': (True, Timestamp('2023-06-04 19:19:24'))}

